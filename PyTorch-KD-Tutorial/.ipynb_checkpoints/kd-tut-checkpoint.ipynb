{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3e566d91-b28f-40e7-a719-1d38dc942ce5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "# Check if GPU is available, and if not, use the CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b823571-1bb4-4944-8390-c87d01fc6f7f",
   "metadata": {},
   "source": [
    "Loading CIFAR-10\n",
    "================\n",
    "\n",
    "CIFAR-10 is a popular image dataset with ten classes. Our objective is\n",
    "to predict one of the following classes for each input image.\n",
    "\n",
    "![Example of CIFAR-10\n",
    "images](https://pytorch.org/tutorials//../_static/img/cifar10.png){.align-center}\n",
    "\n",
    "The input images are RGB, so they have 3 channels and are 32x32 pixels.\n",
    "Basically, each image is described by 3 x 32 x 32 = 3072 numbers ranging\n",
    "from 0 to 255. A common practice in neural networks is to normalize the\n",
    "input, which is done for multiple reasons, including avoiding saturation\n",
    "in commonly used activation functions and increasing numerical\n",
    "stability. Our normalization process consists of subtracting the mean\n",
    "and dividing by the standard deviation along each channel. The tensors\n",
    "\\\"mean=\\[0.485, 0.456, 0.406\\]\\\" and \\\"std=\\[0.229, 0.224, 0.225\\]\\\"\n",
    "were already computed, and they represent the mean and standard\n",
    "deviation of each channel in the predefined subset of CIFAR-10 intended\n",
    "to be the training set. Notice how we use these values for the test set\n",
    "as well, without recomputing the mean and standard deviation from\n",
    "scratch. This is because the network was trained on features produced by\n",
    "subtracting and dividing the numbers above, and we want to maintain\n",
    "consistency. Furthermore, in real life, we would not be able to compute\n",
    "the mean and standard deviation of the test set since, under our\n",
    "assumptions, this data would not be accessible at that point.\n",
    "\n",
    "As a closing point, we often refer to this held-out set as the\n",
    "validation set, and we use a separate set, called the test set, after\n",
    "optimizing a model\\'s performance on the validation set. This is done to\n",
    "avoid selecting a model based on the greedy and biased optimization of a\n",
    "single metric.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ec9df05d-2752-4057-8443-a46326294084",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Below we are preprocessing data for CIFAR-10. We use an arbitrary batch size of 128.\n",
    "transforms_cifar = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Loading the CIFAR-10 dataset:\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transforms_cifar)\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transforms_cifar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5f610fd7-6475-4f86-8728-90bee952e326",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=2)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da4162b-d19f-4560-924b-831cfe9c19af",
   "metadata": {},
   "source": [
    "Defining model classes and utility functions\n",
    "============================================\n",
    "\n",
    "Next, we need to define our model classes. Several user-defined\n",
    "parameters need to be set here. We use two different architectures,\n",
    "keeping the number of filters fixed across our experiments to ensure\n",
    "fair comparisons. Both architectures are Convolutional Neural Networks\n",
    "(CNNs) with a different number of convolutional layers that serve as\n",
    "feature extractors, followed by a classifier with 10 classes. The number\n",
    "of filters and neurons is smaller for the students"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bf4f6e6e-c421-4fee-ae5a-c46df025239b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deeper neural network class to be used as teacher:\n",
    "class DeepNN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(DeepNN, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(2048, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# Lightweight neural network class to be used as student:\n",
    "class LightNN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(LightNN, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(16, 16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(1024, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520fcd97-0343-41ea-a38d-46563ba6088e",
   "metadata": {},
   "source": [
    "We employ 2 functions to help us produce and evaluate the results on our\n",
    "original classification task. One function is called `train` and takes\n",
    "the following arguments:\n",
    "\n",
    "-   `model`: A model instance to train (update its weights) via this\n",
    "    function.\n",
    "-   `train_loader`: We defined our `train_loader` above, and its job is\n",
    "    to feed the data into the model.\n",
    "-   `epochs`: How many times we loop over the dataset.\n",
    "-   `learning_rate`: The learning rate determines how large our steps\n",
    "    towards convergence should be. Too large or too small steps can be\n",
    "    detrimental.\n",
    "-   `device`: Determines the device to run the workload on. Can be\n",
    "    either CPU or GPU depending on availability.\n",
    "\n",
    "Our test function is similar, but it will be invoked with `test_loader`\n",
    "to load images from the test set.\n",
    "\n",
    "![Train both networks with Cross-Entropy. The student will be used as a\n",
    "baseline:](https://pytorch.org/tutorials//../_static/img/knowledge_distillation/ce_only.png){.align-center"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4814c3-a238-4ea5-b2c9-ea20c53db1d5",
   "metadata": {},
   "source": [
    "Cross-entropy runs\n",
    "==================\n",
    "\n",
    "For reproducibility, we need to set the torch manual seed. We train\n",
    "networks using different methods, so to compare them fairly, it makes\n",
    "sense to initialize the networks with the same weights. Start by\n",
    "training the teacher network using cross-entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0aab80ee-2432-4682-ae07-8fdb8fcf9cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, epochs, learning_rate, device):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            # inputs: A collection of batch_size images\n",
    "            # labels: A vector of dimensionality batch_size with integers denoting class of each image\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # outputs: Output of the network for the collection of images. A tensor of dimensionality batch_size x num_classes\n",
    "            # labels: The actual labels of the images. Vector of dimensionality batch_size\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss / len(train_loader)}\")\n",
    "\n",
    "def test(model, test_loader, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
    "    return accurac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b44aa0a8-0b56-4e90-bdc1-d4908409c297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1, Loss: 1.3284041829731152\n",
      "Test Accuracy: 65.00%\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'accurac' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m nn_deep \u001b[38;5;241m=\u001b[39m DeepNN(num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      3\u001b[0m train(nn_deep, train_loader, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m----> 4\u001b[0m test_accuracy_deep \u001b[38;5;241m=\u001b[39m \u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnn_deep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Instantiate the lightweight network:\u001b[39;00m\n\u001b[0;32m      7\u001b[0m torch\u001b[38;5;241m.\u001b[39mmanual_seed(\u001b[38;5;241m42\u001b[39m)\n",
      "Cell \u001b[1;32mIn[28], line 46\u001b[0m, in \u001b[0;36mtest\u001b[1;34m(model, test_loader, device)\u001b[0m\n\u001b[0;32m     44\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m*\u001b[39m correct \u001b[38;5;241m/\u001b[39m total\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 46\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43maccurac\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'accurac' is not defined"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "nn_deep = DeepNN(num_classes=10).to(device)\n",
    "train(nn_deep, train_loader, epochs=1, learning_rate=0.001, device=device)\n",
    "test_accuracy_deep = test(nn_deep, test_loader, device)\n",
    "\n",
    "# Instantiate the lightweight network:\n",
    "torch.manual_seed(42)\n",
    "nn_light = LightNN(num_classes=10).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5e03f2f6-0593-46f2-9757-a81c14cabea1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e9b38633-8fa0-497f-8d89-5198617fec8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2bb64e-e35c-4cca-8ecc-7855d8127b71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
