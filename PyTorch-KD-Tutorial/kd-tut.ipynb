{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3e566d91-b28f-40e7-a719-1d38dc942ce5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "# Check if GPU is available, and if not, use the CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b823571-1bb4-4944-8390-c87d01fc6f7f",
   "metadata": {},
   "source": [
    "Loading CIFAR-10\n",
    "================\n",
    "\n",
    "CIFAR-10 is a popular image dataset with ten classes. Our objective is\n",
    "to predict one of the following classes for each input image.\n",
    "\n",
    "![Example of CIFAR-10\n",
    "images](https://pytorch.org/tutorials//../_static/img/cifar10.png){.align-center}\n",
    "\n",
    "The input images are RGB, so they have 3 channels and are 32x32 pixels.\n",
    "Basically, each image is described by 3 x 32 x 32 = 3072 numbers ranging\n",
    "from 0 to 255. A common practice in neural networks is to normalize the\n",
    "input, which is done for multiple reasons, including avoiding saturation\n",
    "in commonly used activation functions and increasing numerical\n",
    "stability. Our normalization process consists of subtracting the mean\n",
    "and dividing by the standard deviation along each channel. The tensors\n",
    "\\\"mean=\\[0.485, 0.456, 0.406\\]\\\" and \\\"std=\\[0.229, 0.224, 0.225\\]\\\"\n",
    "were already computed, and they represent the mean and standard\n",
    "deviation of each channel in the predefined subset of CIFAR-10 intended\n",
    "to be the training set. Notice how we use these values for the test set\n",
    "as well, without recomputing the mean and standard deviation from\n",
    "scratch. This is because the network was trained on features produced by\n",
    "subtracting and dividing the numbers above, and we want to maintain\n",
    "consistency. Furthermore, in real life, we would not be able to compute\n",
    "the mean and standard deviation of the test set since, under our\n",
    "assumptions, this data would not be accessible at that point.\n",
    "\n",
    "As a closing point, we often refer to this held-out set as the\n",
    "validation set, and we use a separate set, called the test set, after\n",
    "optimizing a model\\'s performance on the validation set. This is done to\n",
    "avoid selecting a model based on the greedy and biased optimization of a\n",
    "single metric.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ec9df05d-2752-4057-8443-a46326294084",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Below we are preprocessing data for CIFAR-10. We use an arbitrary batch size of 128.\n",
    "transforms_cifar = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Loading the CIFAR-10 dataset:\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transforms_cifar)\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transforms_cifar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5f610fd7-6475-4f86-8728-90bee952e326",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=2)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da4162b-d19f-4560-924b-831cfe9c19af",
   "metadata": {},
   "source": [
    "Defining model classes and utility functions\n",
    "============================================\n",
    "\n",
    "Next, we need to define our model classes. Several user-defined\n",
    "parameters need to be set here. We use two different architectures,\n",
    "keeping the number of filters fixed across our experiments to ensure\n",
    "fair comparisons. Both architectures are Convolutional Neural Networks\n",
    "(CNNs) with a different number of convolutional layers that serve as\n",
    "feature extractors, followed by a classifier with 10 classes. The number\n",
    "of filters and neurons is smaller for the students"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bf4f6e6e-c421-4fee-ae5a-c46df025239b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deeper neural network class to be used as teacher:\n",
    "class DeepNN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(DeepNN, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(2048, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# Lightweight neural network class to be used as student:\n",
    "class LightNN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(LightNN, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(16, 16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(1024, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520fcd97-0343-41ea-a38d-46563ba6088e",
   "metadata": {},
   "source": [
    "We employ 2 functions to help us produce and evaluate the results on our\n",
    "original classification task. One function is called `train` and takes\n",
    "the following arguments:\n",
    "\n",
    "-   `model`: A model instance to train (update its weights) via this\n",
    "    function.\n",
    "-   `train_loader`: We defined our `train_loader` above, and its job is\n",
    "    to feed the data into the model.\n",
    "-   `epochs`: How many times we loop over the dataset.\n",
    "-   `learning_rate`: The learning rate determines how large our steps\n",
    "    towards convergence should be. Too large or too small steps can be\n",
    "    detrimental.\n",
    "-   `device`: Determines the device to run the workload on. Can be\n",
    "    either CPU or GPU depending on availability.\n",
    "\n",
    "Our test function is similar, but it will be invoked with `test_loader`\n",
    "to load images from the test set.\n",
    "\n",
    "![Train both networks with Cross-Entropy. The student will be used as a\n",
    "baseline:](https://pytorch.org/tutorials//../_static/img/knowledge_distillation/ce_only.png){.align-center"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4814c3-a238-4ea5-b2c9-ea20c53db1d5",
   "metadata": {},
   "source": [
    "Cross-entropy runs\n",
    "==================\n",
    "\n",
    "For reproducibility, we need to set the torch manual seed. We train\n",
    "networks using different methods, so to compare them fairly, it makes\n",
    "sense to initialize the networks with the same weights. Start by\n",
    "training the teacher network using cross-entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0aab80ee-2432-4682-ae07-8fdb8fcf9cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, epochs, learning_rate, device):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            # inputs: A collection of batch_size images\n",
    "            # labels: A vector of dimensionality batch_size with integers denoting class of each image\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # outputs: Output of the network for the collection of images. A tensor of dimensionality batch_size x num_classes\n",
    "            # labels: The actual labels of the images. Vector of dimensionality batch_size\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss / len(train_loader)}\")\n",
    "\n",
    "def test(model, test_loader, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b44aa0a8-0b56-4e90-bdc1-d4908409c297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 1.3290103455943525\n",
      "Epoch 2/10, Loss: 0.8618725254712507\n",
      "Epoch 3/10, Loss: 0.6830734239362389\n",
      "Epoch 4/10, Loss: 0.5315783902659745\n",
      "Epoch 5/10, Loss: 0.4122244017127225\n",
      "Epoch 6/10, Loss: 0.3005805501852499\n",
      "Epoch 7/10, Loss: 0.21594502018464495\n",
      "Epoch 8/10, Loss: 0.1664198536873626\n",
      "Epoch 9/10, Loss: 0.1377350527107182\n",
      "Epoch 10/10, Loss: 0.11376107812327954\n",
      "Test Accuracy: 75.72%\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "nn_deep = DeepNN(num_classes=10).to(device)\n",
    "train(nn_deep, train_loader, epochs=10, learning_rate=0.001, device=device)\n",
    "test_accuracy_deep = test(nn_deep, test_loader, device)\n",
    "\n",
    "# Instantiate the lightweight network:\n",
    "torch.manual_seed(42)\n",
    "nn_light = LightNN(num_classes=10).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5e03f2f6-0593-46f2-9757-a81c14cabea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "new_nn_light = LightNN(num_classes=10).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf5deb7-0262-4081-aaa2-77eca88d1c05",
   "metadata": {},
   "source": [
    "To ensure we have created a copy of the first network, we inspect the\n",
    "norm of its first layer. If it matches, then we are safe to conclude\n",
    "that the networks are indeed the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8b2bb64e-e35c-4cca-8ecc-7855d8127b71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm of 1st layer of nn_light: 2.327361822128296\n",
      "Norm of 1st layer of new_nn_light: 2.327361822128296\n"
     ]
    }
   ],
   "source": [
    "# Print the norm of the first layer of the initial lightweight model\n",
    "print(\"Norm of 1st layer of nn_light:\", torch.norm(nn_light.features[0].weight).item())\n",
    "# Print the norm of the first layer of the new lightweight model\n",
    "print(\"Norm of 1st layer of new_nn_light:\", torch.norm(new_nn_light.features[0].weight).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21045bf5-2099-4172-8dbc-48ecbb3b110e",
   "metadata": {},
   "source": [
    "## print the total numbers of parameters of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "565a333a-a0f5-43d7-96b6-d91ea36e9729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepNN parameters: 1,186,986\n",
      "LightNN parameters: 267,738\n"
     ]
    }
   ],
   "source": [
    "total_params_deep = \"{:,}\".format(sum(p.numel() for p in nn_deep.parameters()))\n",
    "print(f\"DeepNN parameters: {total_params_deep}\")\n",
    "total_params_light = \"{:,}\".format(sum(p.numel() for p in nn_light.parameters()))\n",
    "print(f\"LightNN parameters: {total_params_light}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77064031-9436-415c-8c15-67940763e76e",
   "metadata": {},
   "source": [
    "## Train and test lightweight network with cross entropy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "551e793f-cba5-41f5-acac-c0a28aeb2221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 1.4703492529861761\n",
      "Epoch 2/10, Loss: 1.1586465205987702\n",
      "Epoch 3/10, Loss: 1.0233277681538515\n",
      "Epoch 4/10, Loss: 0.9199803726142629\n",
      "Epoch 5/10, Loss: 0.84628275547491\n",
      "Epoch 6/10, Loss: 0.7804365962972422\n",
      "Epoch 7/10, Loss: 0.7133219540881379\n",
      "Epoch 8/10, Loss: 0.6576626432673706\n",
      "Epoch 9/10, Loss: 0.6038828446432147\n",
      "Epoch 10/10, Loss: 0.5523658263713808\n",
      "Test Accuracy: 69.91%\n"
     ]
    }
   ],
   "source": [
    "train(nn_light, train_loader, epochs=10, learning_rate=0.001, device=device)\n",
    "test_accuracy_light_ce = test(nn_light, test_loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598c48b5-0fd4-440f-897f-151c8149b2d0",
   "metadata": {},
   "source": [
    "As we can see, based on test accuracy, we can now compare the deeper network that is to be used as a teacher with the lightweight network that is our supposed student. So far, our student has not intervened with the teacher, therefore this performance is achieved by the student itself. The metrics so far can be seen with the following lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4e238ba8-57b8-49df-975f-98615b1b7e23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teacher accuracy: 75.72%\n",
      "Student accuracy: 69.91%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Teacher accuracy: {test_accuracy_deep:.2f}%\")\n",
    "print(f\"Student accuracy: {test_accuracy_light_ce:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0535152-d623-4c76-b95b-7802e14835b2",
   "metadata": {},
   "source": [
    "Knowledge distillation run\n",
    "==========================\n",
    "\n",
    "Now let\\'s try to improve the test accuracy of the student network by\n",
    "incorporating the teacher. Knowledge distillation is a straightforward\n",
    "technique to achieve this, based on the fact that both networks output a\n",
    "probability distribution over our classes. Therefore, the two networks\n",
    "share the same number of output neurons. The method works by\n",
    "incorporating an additional loss into the traditional cross entropy\n",
    "loss, which is based on the softmax output of the teacher network. The\n",
    "assumption is that the output activations of a properly trained teacher\n",
    "network carry additional information that can be leveraged by a student\n",
    "network during training. The original work suggests that utilizing\n",
    "ratios of smaller probabilities in the soft targets can help achieve the\n",
    "underlying objective of deep neural networks, which is to create a\n",
    "similarity structure over the data where similar objects are mapped\n",
    "closer together. For example, in CIFAR-10, a truck could be mistaken for\n",
    "an automobile or airplane, if its wheels are present, but it is less\n",
    "likely to be mistaken for a dog. Therefore, it makes sense to assume\n",
    "that valuable information resides not only in the top prediction of a\n",
    "properly trained model but in the entire output distribution. However,\n",
    "cross entropy alone does not sufficiently exploit this information as\n",
    "the activations for non-predicted classes tend to be so small that\n",
    "propagated gradients do not meaningfully change the weights to construct\n",
    "this desirable vector space.\n",
    "\n",
    "As we continue defining our first helper function that introduces a\n",
    "teacher-student dynamic, we need to include a few extra parameters:\n",
    "\n",
    "-   `T`: Temperature controls the smoothness of the output\n",
    "    distributions. Larger `T` leads to smoother distributions, thus\n",
    "    smaller probabilities get a larger boost.\n",
    "-   `soft_target_loss_weight`: A weight assigned to the extra objective\n",
    "    we\\'re about to include.\n",
    "-   `ce_loss_weight`: A weight assigned to cross-entropy. Tuning these\n",
    "    weights pushes the network towards optimizing for either objective.\n",
    "\n",
    "![Distillation loss is calculated from the logits of the networks. It\n",
    "only returns gradients to the\n",
    "student:](https://pytorch.org/tutorials//../_static/img/knowledge_distillation/distillation_output_loss.png){.align-center}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d03e2a1-0d9f-4022-b69e-488f7c8a37dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
